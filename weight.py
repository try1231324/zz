import torch
import os
import numpy as np

# 读取 .pth 文件路径
pth_file = '/cwj/training-mixed-precision-quantized-networks/results/Imagenet/mobilenet_cifar100_4bit_icn160PLPACT2/full_int_model_with_quant_params.pth'
save_dir = 'layer_h_files4'
output_file = 'merged_layers.h'

os.makedirs(save_dir, exist_ok=True)

# 设置 Z_w 偏移为 8
Z_w = 8

# 加载 pth
data = torch.load(pth_file, map_location='cpu')
all_layers_params = data['quant_params']

def combine_4bit_to_8bit(weight_4bit):
    """将每两个4bit数合并为一个8bit数"""
    weight_4bit = weight_4bit.flatten()
    combined_weight = []
    for i in range(0, len(weight_4bit), 2):
        low_4bit = weight_4bit[i] & 0x0F
        high_4bit = (weight_4bit[i + 1] & 0x0F) << 4
        combined_8bit = low_4bit | high_4bit
        combined_weight.append(combined_8bit)
    return np.array(combined_weight, dtype=np.uint8)

# 生成每层的 .h 文件
for i, layer in enumerate(all_layers_params):
    filename = os.path.join(save_dir, f'layer_{i}.h')
    with open(filename, 'w') as f:
        f.write(f"// Layer {i} quantized parameters\n\n")

        conv = layer.get('quant_conv')
        act = layer.get('quant_act')

        # 如果是卷积层
        if conv:
            for param_name in ['in_channels', 'out_channels', 'kernel_size', 'stride', 'padding', 'dilation', 'output_padding', 'groups']:
                val = conv.get(param_name)
                if val is not None:
                    f.write(f"// {param_name.upper()}: {val}\n")

            # 权重
            weight = conv.get('weight')
            if weight is not None:
                weight = weight.astype(np.int8)
                weight = weight + Z_w
                weight = np.clip(weight, 0, 15)
                combined_weight = combine_4bit_to_8bit(weight)

                f.write(f"\nconst uint8_t WEIGHT_{i}[{combined_weight.size}] = {{\n")
                for idx, val in enumerate(combined_weight.flatten()):
                    f.write(f"{val}, ")
                    if (idx + 1) % 16 == 0:
                        f.write("\n")
                f.write("\n};\n")

            # 偏置
            bias = conv.get('bias')
            if bias is not None:
                bias = bias.astype(np.int32)
                f.write(f"\nconst int32_t BIAS_{i}[{bias.size}] = {{\n")
                for idx, val in enumerate(bias.flatten()):
                    f.write(f"{val}, ")
                    if (idx + 1) % 16 == 0:
                        f.write("\n")
                f.write("\n};\n")

            # 写入宏定义
            f.write(f"\n#define Z_W_{i} {Z_w}\n")

            # Z_IN / Z_OUT
            Z_in_val = int(conv.get('Z_IN', 0))
            Z_out_val = int(conv.get('Z_OUT', 0))
            f.write(f"#define Z_IN_{i} {Z_in_val}\n")
            f.write(f"#define Z_OUT_{i} {Z_out_val}\n")

        # 如果是激活层
        if act:
            M_ZERO = act.get('M_ZERO')
            N_ZERO = act.get('N_ZERO')
            clip_val = act.get('clip_val')

            # if M_ZERO is not None:
            #     M_ZERO = M_ZERO.astype(np.float32)
            #     f.write(f"\nconst float M_ZERO_{i}[{M_ZERO.size}] = {{ ")
            #     f.write(", ".join(f"{x:.6f}" for x in M_ZERO.flatten()))
            #     f.write(" };\n")

            # if N_ZERO is not None:
            #     N_ZERO = N_ZERO.astype(np.float32)
            #     f.write(f"\nconst float N_ZERO_{i}[{N_ZERO.size}] = {{ ")
            #     f.write(", ".join(f"{x:.6f}" for x in N_ZERO.flatten()))
            #     f.write(" };\n")
            if M_ZERO is not None:
    # 转换为 Q31
                M_ZERO_q31 = np.round(M_ZERO * (1 << 31)).astype(np.int32)
                f.write(f"\nconst int32_t M_ZERO_{i}[{M_ZERO_q31.size}] = {{\n")
                for idx, val in enumerate(M_ZERO_q31.flatten()):
                    f.write(f"{val}, ")
                    if (idx + 1) % 8 == 0:
                        f.write("\n")
                f.write("};\n")
            if N_ZERO is not None:
    # 转换为移位量: 1/128 -> shift=7, 1/256 -> shift=8, etc.
                N_ZERO_shift = np.round(-np.log2(N_ZERO)).astype(np.int8)
                f.write(f"\nconst int8_t N_ZERO_{i}[{N_ZERO_shift.size}] = {{\n")
                for idx, val in enumerate(N_ZERO_shift.flatten()):
                    f.write(f"{val}, ")
                    if (idx + 1) % 16 == 0:
                        f.write("\n")
                f.write("};\n")

            if clip_val is not None:
                f.write(f"\n#define CLIP_VAL_{i} {float(clip_val):.6f}\n")

print(f"✅ Export completed. {len(all_layers_params)} files saved to: {save_dir}")

# 合并所有 .h 文件
h_files = [f for f in os.listdir(save_dir) if f.endswith('.h')]
h_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))

with open(output_file, 'w') as outfile:
    outfile.write("// Merged layers header file\n")
    outfile.write("// Generated by combined_script.py\n\n")

    for h_file in h_files:
        file_path = os.path.join(save_dir, h_file)
        with open(file_path, 'r') as infile:
            outfile.write(f"\n// ===== {h_file} =====\n")
            outfile.write(infile.read())

print(f"✅ All files have been merged into {output_file}")
